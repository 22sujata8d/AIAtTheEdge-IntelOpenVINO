The Model Optimizer
-------------------

It converts a model for use with the Inference Engine, including the **improvements to size and speed**.
The **Model Optimizer** helps **convert** models in multiple different frameworks to an **Intermediate Representation**, which is **used** 
with the Inference Engine. If a model is not one of the pre-converted models in the Pre-Trained Models OpenVINO™ provides, 
it is a required step to move onto the Inference Engine.

**Developer Guide**: [The Model Optimizer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)

------

### Local Configuration

Configuring the Model Optimizer is pretty straight forward for your local machine, given that you already have OpenVINO™ 
installed. You can navigate to your **OpenVINO™ install directory** first, which is usually **`/opt/intel/openvino`**.
Then, head to **`/deployment_tools/model_optimizer/install_prerequisites`**, and **run the `install_prerequisites.sh`**
script therein.

------

### Optimizations Techniques

* **Quantization**<br>
  Quantization refers to the process of reducing the number of bits that represent a number. In the context of deep learning, 
  the predominant numerical format used for research and for deployment has so far been 32-bit floating point, or FP32. However
  , the desire for reduced bandwidth and compute requirements of deep learning models has driven research into using 
  lower-precision numerical formats. It has been extensively demonstrated that weights and activations can be represented
  using 8-bit integers (or INT8) without incurring significant loss in accuracy.<br>
  Quantization is a common method used for running models at the edge.
  
  **Resource**: https://nervanasystems.github.io/distiller/quantization.html
  
* **Freezing**<br>
  Freezing in this context is used for TensorFlow models. Freezing TensorFlow models will remove certain operations 
  and metadata only needed for training, such as those related to backpropagation. Freezing a TensorFlow model is 
  usually a good idea whether before performing direct inference or converting with the Model Optimizer.

* **Fusion**<br>
  Fusion relates to combining multiple layer operations into a single operation. For example, a batch normalization layer,
  activation layer, and convolutional layer could be combined into a single operation. This can be particularly useful for
  GPU inference, where the separate operations may occur on separate GPU kernels, while a fused operation occurs on one
  kernel, thereby incurring less overhead in switching from one kernel to the next.
  
**Developer Documentation**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html

----------

### Intermediate Representations<br>

The Intermediate Representation is a model where specific layers of supported deep learning frameworks are replaced
with layers in the “dialect” of the Inference Engine.<br>
Intermediate Representations (IRs) are the OpenVINO™ Toolkit’s standard structure and naming for 
neural network architectures. A **Conv2D layer in TensorFlow**, **Convolution layer in Caffe**, or **Conv layer
in ONNX** are all **converted** into a **Convolution layer in an IR.**
The IR is able to be loaded directly into the Inference Engine,
and is actually made of two output files from the Model Optimizer:<br>
* **`XML file`**: holds the model architecture and other important metadata.
* **`Binary file`**: holds weights and biases in a binary format.
One need both of these files in order to run inference Any desired optimizations will have occurred
while this is generated by the Model Optimizer, such as changes to precision. Certain precisions can be generated with the
**--data_type argument**, which is usually FP32 by default.<br>

**Converting a model to IR**:
https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html

----------

### Using Model Optimizer with Tensorflow Models
Once the Model Optimizer is configured, the next thing to do with a TensorFlow model is to determine whether to use a frozen or unfrozen model. You can either freeze your model, or use the separate instructions in the documentation to convert a non-frozen model. <br>
TensorFlow models can vary for what additional steps are needed by model type, being unfrozen or frozen, or being from the TensorFlow Detection Model Zoo. **Unfrozen models** usually need the **--mean_values** and **--scale** parameters fed to the Model Optimizer, while the **frozen models** from the Object Detection Model Zoo **don’t need those parameters**. However, the **frozen models** will **need TensorFlow-specific parameters** like **--tensorflow_use_custom_operations_config** and **--tensorflow_object_detection_api_pipeline_config**. Also, **--reverse_input_channels is usually needed, as TF model zoo models are trained on RGB images, while OpenCV usually loads as BGR**. Certain models, like YOLO, DeepSpeech, and more, have their own separate pages.<br>

**Developer documentation for Converting TensorFlow Models**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html

### Using Model Optimizer with Caffe Models
The process for converting a Caffe model is fairly similar to the TensorFlow one, although **there’s nothing about freezing the model this time around, since that’s a TensorFlow concept**. Caffe does have some differences in the set of supported model architectures. Additionally, Caffe models need to feed both the `.caffemodel file`, as well as a `.prototxt file`, into the Model Optimizer. If they have the same name, only the model needs to be directly input as an argument, while if the .prototxt file has a different name than the model, it should be fed in with `--input_proto` as well.

**Developer documentation for Converting Caffe Models**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html

### Using Model Optimizer with ONNX Models
The process for converting an ONNX model is again quite similar to the previous two, although ONNX does not have any ONNX-specific arguments to the Model Optimizer. So, you’ll only have the general arguments for items like changing the precision.

**Developer documentation for Converting ONNX Models**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html 

----------

### Cutting Parts of The Model
Cutting a model is mostly applicable for TensorFlow models. Some common reasons for cutting are:
* The model has pre- or post-processing parts that don’t translate to existing Inference Engine layers.
* The model has a training part that is convenient to be kept in the model, but is not used during inference.
* The model is too complex with many unsupported operations, so the complete model cannot be converted in one shot.
* The model is one of the supported SSD models. In this case, you need to cut a post-processing part off.
There could be a problem with model conversion in the Model Optimizer or with inference in the Inference Engine. To localize the issue, cutting the model could help to find the problem
There’s two main command line arguments to use for cutting a model with the Model Optimizer, named intuitively as `--input` and `--output`, where they are used to feed in the layer names that should be either the new entry or exit points of the model.

**Developer Documentation**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html

----------
### When A Layer Is Not Supported By The Model Optimizer
* One potential solution is the use of **custom layers**. 
* Another solution is actually **running the given unsupported layer in its original framework**. For example, you could potentially use TensorFlow to load and process the inputs and outputs for a specific layer you built in that framework, if it isn’t supported with the Model Optimizer.
* Lastly, there are also unsupported layers for certain hardware, that you may run into when working with the Inference Engine. In this case, there are sometimes extensions available that can add support.

**Supported Layers List**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html

----------
### Custom Layers
To actually add custom layers, there are a few differences depending on the original model framework. In both TensorFlow and Caffe, the first option is to register the custom layers as extensions to the Model Optimizer.

For Caffe, the second option is to register the layers as Custom, then use Caffe to calculate the output shape of the layer. You’ll need Caffe on your system to do this option.

For TensorFlow, its second option is to actually replace the unsupported subgraph with a different subgraph. The final TensorFlow option is to actually offload the computation of the subgraph back to TensorFlow during inference.

**Developer Documentation**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html

----------

### Further Readings
* [Caffee](https://caffe.berkeleyvision.org/)
* [Tensorflow](https://www.tensorflow.org/)
* [MXNET](https://mxnet.apache.org/)
* [ONNX](https://onnx.ai/)
* [Kaldi](https://kaldi-asr.org/doc/dnn.html)
