The Model Optimizer
-------------------

It converts a model for use with the Inference Engine, including the **improvements to size and speed**.
The **Model Optimizer** helps **convert** models in multiple different frameworks to an **Intermediate Representation**, which is **used** 
with the Inference Engine. If a model is not one of the pre-converted models in the Pre-Trained Models OpenVINO™ provides, 
it is a required step to move onto the Inference Engine.

**Developer Guide**: [The Model Optimizer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)

------

### Local Configuration

Configuring the Model Optimizer is pretty straight forward for your local machine, given that you already have OpenVINO™ 
installed. You can navigate to your **OpenVINO™ install directory** first, which is usually **`/opt/intel/openvino`**.
Then, head to **`/deployment_tools/model_optimizer/install_prerequisites`**, and **run the `install_prerequisites.sh`**
script therein.

------

### Optimizations Techniques

* **Quantization**<br>
  Quantization refers to the process of reducing the number of bits that represent a number. In the context of deep learning, 
  the predominant numerical format used for research and for deployment has so far been 32-bit floating point, or FP32. However
  , the desire for reduced bandwidth and compute requirements of deep learning models has driven research into using 
  lower-precision numerical formats. It has been extensively demonstrated that weights and activations can be represented
  using 8-bit integers (or INT8) without incurring significant loss in accuracy.<br>
  Quantization is a common method used for running models at the edge.
  
  **Resource**: https://nervanasystems.github.io/distiller/quantization.html
  
* **Freezing**<br>
  Freezing in this context is used for TensorFlow models. Freezing TensorFlow models will remove certain operations 
  and metadata only needed for training, such as those related to backpropagation. Freezing a TensorFlow model is 
  usually a good idea whether before performing direct inference or converting with the Model Optimizer.

* **Fusion**<br>
  Fusion relates to combining multiple layer operations into a single operation. For example, a batch normalization layer,
  activation layer, and convolutional layer could be combined into a single operation. This can be particularly useful for
  GPU inference, where the separate operations may occur on separate GPU kernels, while a fused operation occurs on one
  kernel, thereby incurring less overhead in switching from one kernel to the next.
  
**Extra Reading**: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html

----------

### Intermediate Representations<br>

The Intermediate Representation is a model where specific layers of supported deep learning frameworks are replaced
with layers in the “dialect” of the Inference Engine.<br>
Intermediate Representations (IRs) are the OpenVINO™ Toolkit’s standard structure and naming for 
neural network architectures. A **Conv2D layer in TensorFlow**, **Convolution layer in Caffe**, or **Conv layer
in ONNX** are all **converted** into a **Convolution layer in an IR.**
The IR is able to be loaded directly into the Inference Engine,
and is actually made of two output files from the Model Optimizer:<br>
* **`XML file`**: holds the model architecture and other important metadata.
* **`Binary file`**: holds weights and biases in a binary format.
One need both of these files in order to run inference Any desired optimizations will have occurred
while this is generated by the Model Optimizer, such as changes to precision. Certain precisions can be generated with the
**--data_type argument**, which is usually FP32 by default.<br>

**Converting a model to IR**:
https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html

----------

### Further Readings
* [Caffee](https://caffe.berkeleyvision.org/)
* [Tensorflow](https://www.tensorflow.org/)
* [MXNET](https://mxnet.apache.org/)
* [ONNX](https://onnx.ai/)
* [Kaldi](https://kaldi-asr.org/doc/dnn.html)
